{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngram Model (updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the perfect result which is trained on all the dataset, this code is trying to separate the train and test dataset to evaluate the result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/apple/Documents/GitHub/Argument-Scoring-System/pretrained_model/labeled_essay_dics.pickle', 'rb') as handle:\n",
    "    label_sents = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute and store and the {sents: label} into one dic\n",
    "all_label_sents = {}\n",
    "for key in label_sents.keys():\n",
    "    this_essay = label_sents[key]\n",
    "    for s in this_essay.keys():\n",
    "        all_label_sents[s] = this_essay[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Original taken from https://github.com/dennybritz/cnn-text-classification-tf\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clear_split_str(s):\n",
    "    strip_s = s.strip()\n",
    "    clear_s = clean_str(strip_s)\n",
    "    s_text = clear_s.split(\" \")\n",
    "    return s_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for key in all_label_sents.keys():\n",
    "    this_value = all_label_sents[key]\n",
    "    clear_key = clear_split_str(key)\n",
    "    X.append(clear_key)\n",
    "    y.append(this_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                   y,\n",
    "                                                   test_size = 0.1,\n",
    "#                                                    shuffle = True\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## separate by class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "claim_list = []\n",
    "premise_list = []\n",
    "major_claim_list = []\n",
    "other_list = []\n",
    "\n",
    "for i in range(0, len(X_train)):\n",
    "    this_value = y_train[i]\n",
    "    clear_key = X_train[i]\n",
    "    if this_value == \"Premise\":\n",
    "        premise_list.append(clear_key)\n",
    "    elif this_value == \"Claim\":\n",
    "        claim_list.append(clear_key)\n",
    "    elif this_value == \"MajorClaim\":\n",
    "        major_claim_list.append(clear_key)\n",
    "    else:\n",
    "        other_list.append(clear_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7077\n"
     ]
    }
   ],
   "source": [
    "# print (claim_list[1:10])\n",
    "print (len(premise_list) + len(claim_list) + len(major_claim_list) + len(other_list) + len(y_test))\n",
    "# print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a bug here, the method of computing pro is wrong \n",
    "def ngram(sent_list, n):\n",
    "    word_num = defaultdict(float)\n",
    "    total_num = 0\n",
    "    for sent in sent_list:\n",
    "        for i in range(len(sent)-n+1):\n",
    "            this_key = ' '.join(sent[i:i+n])\n",
    "            word_num[this_key] = word_num.get(this_key,0) + 1\n",
    "            total_num = total_num + 1\n",
    "    word_pro = {}\n",
    "    for key in word_num:\n",
    "        word_pro[key] = word_num[key]/float(total_num)\n",
    "    return word_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_claim = ngram(claim_list,1)\n",
    "unigram_premise = ngram(premise_list,1)\n",
    "unigram_major_claim = ngram(major_claim_list,1)\n",
    "unigram_others = ngram(other_list,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_claim = ngram(claim_list,2)\n",
    "bigram_premise = ngram(premise_list,2)\n",
    "bigram_major_claim = ngram(major_claim_list,2)\n",
    "bigram_others = ngram(other_list,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_claim = ngram(claim_list,3)\n",
    "trigram_premise = ngram(premise_list,3)\n",
    "trigram_major_claim = ngram(major_claim_list,3)\n",
    "trigram_others = ngram(other_list,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fourgram_claim = ngram(claim_list,4)\n",
    "fourgram_premise = ngram(premise_list,4)\n",
    "fourgram_major_claim = ngram(major_claim_list,4)\n",
    "fourgram_others = ngram(other_list,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the right equation to compute the ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### bug here\n",
    "\n",
    "def ngram1_4(sent_list):\n",
    "    '''\n",
    "    computer the 1 to 4 gram, and return the list\n",
    "    sent_list: list of sents\n",
    "    return: a dic includes one to four grams \n",
    "    '''\n",
    "    # unigram\n",
    "    word_num = defaultdict(float)\n",
    "    total_num = 0\n",
    "    for sent in sent_list:\n",
    "        for i in range(len(sent)-1+1):\n",
    "            this_key = ' '.join(sent[i:i+1])\n",
    "            word_num[this_key] = word_num.get(this_key,0) + 1\n",
    "            total_num = total_num + 1\n",
    "    word_pro = {}\n",
    "    for key in word_num:\n",
    "        word_pro[key] = word_num[key]/float(total_num)\n",
    "        \n",
    "    # bigram\n",
    "    bi_num = defaultdict(float)\n",
    "    for sent in sent_list:\n",
    "        for i in range(len(sent)-2+1):\n",
    "            this_key = ' '.join(sent[i:i+2])\n",
    "            bi_num[this_key] = bi_num.get(this_key,0) + 1\n",
    "    bi_pro = {}\n",
    "    for key in bi_num:\n",
    "        pre_key = key.split()[0]\n",
    "        bi_pro[key] = bi_num[key]/float(word_num[pre_key])\n",
    "    \n",
    "    # trigram\n",
    "    tri_num = defaultdict(float)\n",
    "    for sent in sent_list:\n",
    "        for i in range(len(sent)-3+1):\n",
    "            this_key = ' '.join(sent[i:i+3])\n",
    "            bi_num[this_key] = bi_num.get(this_key,0) + 1\n",
    "    tri_pro = {}\n",
    "    for key in bi_num:\n",
    "        pre_key = key.split()[0:2]\n",
    "        tri_pro[key] = tri_num[key]/float(word_num[pre_key])\n",
    "    \n",
    "    # fourgram\n",
    "    \n",
    "    four_num = defaultdict(float)\n",
    "    for sent in sent_list:\n",
    "        for i in range(len(sent)-4+1):\n",
    "            this_key = ' '.join(sent[i:i+4])\n",
    "            bi_num[this_key] = bi_num.get(this_key,0) + 1\n",
    "    bi_pro = {}\n",
    "    for key in bi_num:\n",
    "        pre_key = key.split()[0:2]\n",
    "        bi_pro[key] = bi_num[key]/float(word_num[pre_key])\n",
    "    \n",
    "    \n",
    "    \n",
    "    gram_dic = {}\n",
    "    gram_dic[1] = word_pro\n",
    "    gram_dic[2] = bi_pro\n",
    "    \n",
    "    return gram_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log,exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_grams(sent, n):\n",
    "    gram_list = []\n",
    "    for i in range(len(sent)-n+1):\n",
    "        gram_list.append(' '.join(sent[i:i+n]))\n",
    "    return gram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "epslon = sys.float_info.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bug here!!!!\n",
    "def sent_prob(keys, pro_dic):\n",
    "    result = 0.0\n",
    "    for key in keys:\n",
    "        result = result + (log(pro_dic.get(key, epslon)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-36.04365338911715"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log(epslon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11.512925464970229"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log(0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_gram(sent):\n",
    "    claim_sum = 0.0\n",
    "    premise_sum = 0.0\n",
    "    major_claim_sum = 0.0\n",
    "    others_sum = 0.0\n",
    "    \n",
    "    p1 = 0.1\n",
    "    p2 = 0.2\n",
    "    p3 = 0.3\n",
    "    p4 = 0.4\n",
    "    \n",
    "    uni_key = get_grams(sent, 1)\n",
    "    claim_sum = claim_sum + p1 * sent_prob(uni_key, unigram_claim)\n",
    "    premise_sum = premise_sum + p1 * sent_prob(uni_key, unigram_premise)\n",
    "    major_claim_sum = major_claim_sum + p1 * sent_prob(uni_key, unigram_major_claim)\n",
    "    others_sum = others_sum + p1 * sent_prob(uni_key, unigram_others)\n",
    "    \n",
    "    bi_key = get_grams(sent, 2)\n",
    "    claim_sum = claim_sum + p2 * sent_prob(bi_key, bigram_claim)\n",
    "    premise_sum = premise_sum + p2 * sent_prob(bi_key, bigram_premise)\n",
    "    major_claim_sum = major_claim_sum + p2 * sent_prob(bi_key, bigram_major_claim)\n",
    "    others_sum = others_sum + p2 * sent_prob(bi_key, bigram_others)   \n",
    "    \n",
    "    tri_key = get_grams(sent, 3)\n",
    "    claim_sum = claim_sum + p3 * sent_prob(tri_key, trigram_claim)\n",
    "    premise_sum = premise_sum + p3 * sent_prob(tri_key, trigram_premise)\n",
    "    major_claim_sum = major_claim_sum + p3 * sent_prob(tri_key, trigram_major_claim)\n",
    "    others_sum = others_sum + p3 * sent_prob(tri_key, trigram_others)      \n",
    "    \n",
    "    four_key = get_grams(sent, 4)\n",
    "    claim_sum = claim_sum + p4 * sent_prob(four_key, fourgram_claim)\n",
    "    premise_sum = premise_sum + p4 * sent_prob(four_key, fourgram_premise)\n",
    "    major_claim_sum = major_claim_sum + p4 * sent_prob(four_key, fourgram_major_claim)\n",
    "    others_sum = others_sum + p4 * sent_prob(four_key, fourgram_others) \n",
    "    \n",
    "    pro_dic = {}\n",
    "    pro_dic['Claim'] = claim_sum\n",
    "    pro_dic['Premise'] = premise_sum\n",
    "    pro_dic['MajorClaim'] = major_claim_sum\n",
    "    pro_dic['Empty'] = others_sum\n",
    "    return pro_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_class(values):\n",
    "    key = max(values, key=lambda i: values[i])\n",
    "    return key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict and error analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# claim_list = []\n",
    "# premise_list = []\n",
    "# major_claim_list = []\n",
    "# other_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for item in X_test:\n",
    "    this_gram = sum_gram(item)\n",
    "    this_value = max_class(sum_gram(item))\n",
    "    y_pred.append(this_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.567796610169\n"
     ]
    }
   ],
   "source": [
    "print (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
