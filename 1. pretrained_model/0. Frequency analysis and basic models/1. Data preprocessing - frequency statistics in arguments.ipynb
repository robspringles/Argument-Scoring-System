{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preprocessing - frequency statistics in arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the frequency of words in claim and premise. The purpose is to find a useful feature in classifying claim and premise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input the labeled sents data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('labeled_essay_dics.pickle', 'rb') as handle:\n",
    "    label_sents = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute and store and the {sents: label} into one dic\n",
    "all_label_sents = {}\n",
    "for key in label_sents.keys():\n",
    "    this_essay = label_sents[key]\n",
    "    for s in this_essay.keys():\n",
    "        all_label_sents[s] = this_essay[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_sents = \"\"\n",
    "for key in all_label_sents.keys():\n",
    "    all_sents = all_sents + \" \" + key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "premise_sents = \"\"\n",
    "major_claim_sents = \"\"\n",
    "claim_sents = \"\"\n",
    "all_claim_sents = \"\"\n",
    "other_arg_sents = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in all_label_sents.keys():\n",
    "    this_value = all_label_sents[key]\n",
    "    if this_value == \"Premise\":\n",
    "        premise_sents = premise_sents + \" \" + key\n",
    "    elif this_value == \"Claim\":\n",
    "        claim_sents = claim_sents + \" \" + key\n",
    "        all_claim_sents = all_claim_sents + \" \" + key\n",
    "    elif this_value == \"MajorClaim\":\n",
    "        major_claim_sents = major_claim_sents + \" \" + key\n",
    "        all_claim_sents = all_claim_sents + \" \" + key\n",
    "    else:\n",
    "        other_arg_sents = other_arg_sents + \" \" + key "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sents cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Original taken from https://github.com/dennybritz/cnn-text-classification-tf\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clear_split_str(s):\n",
    "    strip_s = s.strip()\n",
    "    clear_s = clean_str(strip_s)\n",
    "    s_text = clear_s.split(\" \")\n",
    "    return s_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clear_all_sents = clear_split_str(all_sents)\n",
    "clear_premise_sents = clear_split_str(premise_sents)\n",
    "clear_major_claim_sents = clear_split_str(major_claim_sents)\n",
    "clear_claim_sents = clear_split_str(claim_sents)\n",
    "clear_all_claim_sents = clear_split_str(all_claim_sents)\n",
    "clear_other_arg_sents = clear_split_str(other_arg_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute frequency in claims and premises \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In other words, once you are special and unique, pursuing success is a matter of actively creating the fashion rather than passively waiting for other people to recognize and appreciate you.\n",
      "0\n",
      "It is clear to recognize that the internet service is being provided at a low cost or even free in many countries.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# check the word that appear in \"claim\" and \"premise\" at same time\n",
    "i = 0\n",
    "for key in  all_label_sents.keys():\n",
    "    \n",
    "    if \"recognize\" in clear_split_str(key) and all_label_sents[key] == 'Premise':\n",
    "        print (key)\n",
    "        print (i)\n",
    "        i = i + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FreqDist(clear_premise_sents).most_common(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The code is from https://stackoverflow.com/questions/25653735/freqdist-plot-not-using-the-most-common-words\n",
    "'''\n",
    "\n",
    "def plot_freqdist(fd, num = 0, cumulative = False, title = None):\n",
    "    import pylab\n",
    "    # Set up parameters\n",
    "    if num <= 0:\n",
    "        num = fd.B\n",
    "    # Get samples and frequencies\n",
    "    samples, freq, accu = [], [], 0\n",
    "#     most common words\n",
    "    for s, f in fd.most_common(num):\n",
    "#         least common words \n",
    "#     for s, f in fd.most_common()[-num:]:\n",
    "        accu = accu + f if cumulative else f\n",
    "        samples.append(s)\n",
    "        freq.append(accu)\n",
    "    # Create plot\n",
    "#     pylab.grid(False, color = 'silver')\n",
    "    if title:\n",
    "        pylab.title(title)\n",
    "    pylab.plot(freq, linewidth = 2)\n",
    "    pylab.xticks(range(len(samples)), samples, rotation = 90)\n",
    "    pylab.title(\"The 20 most frequent words and their counts\")\n",
    "    pylab.xlabel('Words')\n",
    "    pylab.ylabel('Cumulative Counts' if cumulative else 'Counts')\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_freqdist(FreqDist(clear_all_sents), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_freqdist(FreqDist(clear_all_sents), 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# computing the differences beween different arguments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words used to build classifiers includes two parts:\n",
    "* most common words\n",
    "* the distinctive words that only appear in one kind of argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although some words appear in different kinds of arguments, but the patter is quite different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_premise = set(clear_premise_sents)\n",
    "set_major_claim_sents = set(clear_major_claim_sents)\n",
    "set_claim_sents = set(clear_claim_sents)\n",
    "set_other_arg_sents = set(other_arg_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniq_premise_words = set_premise - set_major_claim_sents - set_claim_sents - set_other_arg_sents\n",
    "uniq_major_claim_sents = set_major_claim_sents - set_premise - set_claim_sents - set_other_arg_sents\n",
    "uniq_claim_sents = set_claim_sents - set_premise - set_major_claim_sents - set_other_arg_sents\n",
    "uniq_other_arg_sents = set_premise - set_major_claim_sents - set_claim_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print (uniq_claim_sents)\n",
    "FreqDist(clear_claim_sents)['entering']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the result \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear_premise_sents = clear_split_str(premise_sents)\n",
    "# clear_major_claim_sents = clear_split_str(major_claim_sents)\n",
    "# clear_claim_sents = clear_split_str(claim_sents)\n",
    "# clear_all_claim_sents = clear_split_str(all_claim_sents)\n",
    "# clear_other_arg_sents = clear_split_str(other_arg_sents)\n",
    "clear_all_sents = clear_split_str(all_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_words = {}\n",
    "common_words['all'] = clear_all_sents\n",
    "common_words['premise'] = clear_premise_sents\n",
    "common_words['major_claim'] = clear_major_claim_sents\n",
    "common_words['claim'] = clear_claim_sents\n",
    "common_words['all_claim'] = clear_all_claim_sents\n",
    "common_words['other_arg'] = clear_other_arg_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('argument_words.pickle', 'wb') as handle:\n",
    "    pickle.dump(common_words, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
