{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Language Modelling in Hangman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Student Name: Kang Ke\n",
    "\n",
    "Student ID: 745384\n",
    "\n",
    "Python version used: 2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Due date</b>: 5pm, Thursday April 27\n",
    "\n",
    "<b>Submission method</b>: see LMS\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this iPython notebook\n",
    "\n",
    "<b>Late submissions</b>: -20% per day\n",
    "\n",
    "<b>Marks</b>: 5% of mark for class\n",
    "\n",
    "<b>Overview</b>: In this homework, you'll be creating an 'artificial intelligence' player for the classic Hangman word guessing game. You will need to implement several different automatic strategies based on character level language models, ranging from unigram approaches to higher over n-gram models. Your objective is to create an automatic player which makes the fewest mistakes.\n",
    "\n",
    "<b>Materials</b>: See the main class LMS page for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib, Scikit-Learn, and Gensim. In particular, if you are not using a lab computer which already has it installed, we recommend installing all the data for NLTK, since you will need various parts of it to complete this assignment. You can also use any Python built-in packages, but do not use any other 3rd party packages; if your iPython notebook doesn't run on the marker's machine, you will lose marks.  \n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions. You will be marked not only on the correctness of your methods, but also the quality and efficency of your code: in particular, you should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it.\n",
    "\n",
    "<b>Extra credit</b>: Each homework has a task which is optional with respect to getting full marks on the assignment, but that can be used to offset any points lost on this or any other homework assignment (but not the final project or the exam). We recommend you skip over this step on your first pass, and come back if you have time: the amount of effort required to receive full marks (1 point) on an extra credit question will be substantially more than earning the same amount of credit on other parts of the homework.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via LMS. Minor changes and clarifications will be announced in the forum on LMS, we recommend you check the forum regularly.\n",
    "\n",
    "<b>Academic Misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this homework, and we encourge you to discuss it in general terms with other students. However, this ultimately is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the University’s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hangman Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <a href=\"https://en.wikipedia.org/wiki/Hangman_(game)\">Hangman game</a> is a simple game whereby one person thinks of a word, which they keep secret from their opponent, who tries to guess the word one character at a time. The game ends when the opponent makes more than a fixed number of incorrect guesses, or they figure out the secret word before then (in which case they *win*). \n",
    "\n",
    "Here's a simple version of the game, and a method allowing interactive play. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# allowing better python 2 & python 3 compatibility \n",
    "from __future__ import print_function \n",
    "\n",
    "def hangman(secret_word, guesser, max_mistakes=8, verbose=True):\n",
    "    secret_word = secret_word.lower()\n",
    "    mask = ['_'] * len(secret_word)\n",
    "    guessed = set()\n",
    "    if verbose:\n",
    "        print(\"Starting hangman game. Target is\", ' '.join(mask), 'length', len(secret_word))\n",
    "    \n",
    "    mistakes = 0\n",
    "    while mistakes < max_mistakes:\n",
    "        if verbose:\n",
    "            print(\"You have\", (max_mistakes-mistakes), \"attempts remaining.\")\n",
    "        guess = guesser(mask, guessed)\n",
    "\n",
    "        if verbose:\n",
    "            print('Guess is', guess)\n",
    "        if guess in guessed:\n",
    "            if verbose:\n",
    "                print('Already guessed this before.')\n",
    "            mistakes += 1\n",
    "        else:\n",
    "            guessed.add(guess)\n",
    "            if guess in secret_word:\n",
    "                for i, c in enumerate(secret_word):\n",
    "                    if c == guess:\n",
    "                        mask[i] = c\n",
    "                if verbose:\n",
    "                    print('Good guess:', ' '.join(mask))\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print('Sorry, try again.')\n",
    "                mistakes += 1\n",
    "                \n",
    "        if '_' not in mask:\n",
    "            if verbose:\n",
    "                print('Congratulations, you won.')\n",
    "            return mistakes\n",
    "        \n",
    "    if verbose:\n",
    "        print('Out of guesses. The word was', secret_word)    \n",
    "    return mistakes\n",
    "\n",
    "def human(mask, guessed):\n",
    "    print('Enter your guess:')\n",
    "    return raw_input().lower().strip()\n",
    "    #return input().lower().strip() # swap with above for python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play the game interactively using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hangman('whatever', human, 8, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Instructions</b>: We will be using the words occurring in the *Brown* corpus for *training* an artificial intelligence guessing algorithm, and for *evaluating* the quality of the method. Note that we are intentionally making the hangman game hard, as the AI will need to cope with test words that it has not seen before, hence it will need to learn generalisable patterns of characters to make reasonable predictions.\n",
    "\n",
    "Your first task is to compute the unique word types occurring in the *Brown* corpus, using `nltk.corpus.Brown`, selecting only words that are entirely comprised of alphabetic characters, and lowercasing the words. Finally, randomly shuffle (`numpy.random.shuffle`) this collection of word types, and split them into disjoint training and testing sets. The test set should contain 1000 word types, and the rest should be in the training set. Your code should print the sizes of the training and test sets.\n",
    "\n",
    "Feel free to test your own Hangman performance using `hangman(numpy.random.choice(test_set), human, 8, True)`. It is surprisingly difficult (and addictive)!\n",
    "\n",
    "(0.5 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "from numpy.random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "uniq_words = []\n",
    "for w in brown.words():\n",
    "    if w.isalpha():\n",
    "        uniq_words.append(w.lower())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_list = list(set(uniq_words))\n",
    "shuffle(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set is:39234\n",
      "size of test set is:1000\n"
     ]
    }
   ],
   "source": [
    "test_set = word_list[-1000:]\n",
    "trainging_set = word_list[:-1000]\n",
    "print (\"size of training set is:\" + str(len(trainging_set)))\n",
    "print (\"size of test set is:\" + str(len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: To set a baseline, your first *AI* attempt will be a trivial random method. For this you should implement a guessing method, similar to the `human` method above, i.e., using the same input arguments and returning a character. Your method should randomly choose a character from the range `'a'...'z'` after excluding the characters that have already been guessed in the current game (all subsequent AI approaches should also exclude previous guesses). You might want to use `numpy.random.choice` for this purpose.\n",
    "\n",
    "To measure the performance of this (and later) techiques, implement a method that measures the average number of mistakes made by this technique over all the words in the `test_set`. You will want to turn off the printouts for this, using the `verbose=False` option, and increase the cap on the game length to `max_mistakes=26`. Print the average number of mistakes for the random AI, which will become a baseline for the following steps.\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "\n",
    "chr_list = []\n",
    "for i in range(0, 26):\n",
    "    chr_list.append(chr(i+97))\n",
    "\n",
    "def baseline(mask, guessed):\n",
    "    left_list = set(chr_list) - guessed\n",
    "    return choice(list(left_list), 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline is :\n",
      "16.78\n"
     ]
    }
   ],
   "source": [
    "all_mistakes = 0\n",
    "for w in test_set:\n",
    "    mistakes = hangman(w, baseline, max_mistakes=26, verbose=False)\n",
    "    all_mistakes = all_mistakes + mistakes\n",
    "print (\"The baseline is :\")\n",
    "print (float(all_mistakes)/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** As your first real AI, you should train a *unigram* model over the training set.  This requires you to find the frequencies of characters over all training words. Using this model, you should write a guess function that returns the character with the highest probability, after aggregating (summing) the probability of each blank character in the secret word. Print the average number of mistakes the unigram method makes over the test set. (Hint: it should be much lower than for random).\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chr_num = {}\n",
    "chr_num = defaultdict(float)\n",
    "total_chr = 0\n",
    "for w in trainging_set:\n",
    "    for c in w:\n",
    "        chr_num[c] = chr_num.get(c,0) + 1\n",
    "        total_chr = total_chr + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chr_prob = []\n",
    "for w in sorted(chr_num, key=chr_num.get, reverse=True):\n",
    "      chr_prob.append((w,chr_num[w]/float(total_chr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unigram_model(mask, guessed):\n",
    "    guessed_list = list(guessed)\n",
    "    if len(guessed_list) == 0:\n",
    "        return 'e'\n",
    "    else:\n",
    "        index = 0\n",
    "        for item in guessed_list:\n",
    "            for i in range(0,len(chr_prob)): \n",
    "                if item == chr_prob[i][0]:\n",
    "                    if i >= index:\n",
    "                        index = i+1\n",
    "                    break\n",
    "        return chr_prob[index][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of mistakes of unigram is: \n",
      "10.274\n"
     ]
    }
   ],
   "source": [
    "all_mistakes = 0\n",
    "for w in test_set:\n",
    "    mistakes = hangman(w, unigram_model, max_mistakes=26, verbose=False)\n",
    "    all_mistakes = all_mistakes + mistakes\n",
    "print (\"The average number of mistakes of unigram is: \")\n",
    "print (float(all_mistakes)/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** The length of the secret word is an important clue that we might exploit. Different length words tend to have different distributions over characters, e.g., short words are less likely to have suffixes or prefixes. Your job now is to incorporate this idea by conditioning the unigram model on the length of the secret word, i.e., having *different* unigram models for each length of word. You will need to be a little careful at test time, to be robust to the (unlikely) situation that you encounter a word length that you didn't see in training. Create another AI guessing function using this new model, and print its test performance.   \n",
    "\n",
    "(0.5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute the frequency of words length\n",
    "length_dic = defaultdict(float)\n",
    "max_length = 0\n",
    "for w in trainging_set:\n",
    "    length_dic[len(w)] = length_dic.get(len(w),[])+[w]\n",
    "    if len(w) > max_length:\n",
    "        max_length = len(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_unigram(words):\n",
    "    chr_num = defaultdict(float)\n",
    "    total_chr = 0\n",
    "    for w in words:\n",
    "        for c in w:\n",
    "            chr_num[c] = chr_num.get(c,0)+1\n",
    "            total_chr = total_chr + 1\n",
    "    chr_prob = []\n",
    "    for w in sorted(chr_num, key=chr_num.get, reverse=True):\n",
    "        chr_prob.append((w,chr_num[w]/float(total_chr)))\n",
    "    return chr_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unigram_list = []\n",
    "for i in range(1, max_length+1):\n",
    "    words = length_dic[i]\n",
    "    chr_prob = get_unigram(words)\n",
    "    unigram_list.append(chr_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wlen_unigram_model(mask, guessed):\n",
    "    w_prob = unigram_list[len(mask)-1]\n",
    "    guessed_list = list(guessed)\n",
    "    if len(guessed_list) == 0:\n",
    "        return w_prob[0][0]\n",
    "    else:\n",
    "        index = 0\n",
    "        for item in guessed_list:\n",
    "            for i in range(0,len(w_prob)): \n",
    "                if item == w_prob[i][0]:\n",
    "                    if i >= index:\n",
    "                        index = i+1\n",
    "                    break\n",
    "        if index > len(w_prob)-1:\n",
    "            return 'q'\n",
    "        return w_prob[index][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of mistakes of unigram is considered length is:\n",
      "10.267\n"
     ]
    }
   ],
   "source": [
    "all_mistakes = 0\n",
    "for w in test_set:\n",
    "    mistakes = hangman(w, wlen_unigram_model, max_mistakes=26, verbose=False)\n",
    "    all_mistakes = all_mistakes + mistakes\n",
    "print (\"The average number of mistakes of unigram is considered length is:\")\n",
    "print (float(all_mistakes)/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** Now for the main challenge, using a *ngram* language model over characters. The order of characters is obviously important, yet this wasn't incorporated in any of the above models. Knowing that the word has the sequence `n _ s s` is a pretty strong clue that the missing character might be `e`. Similarly the distribution over characters that start and end a word are highly biased (e.g., toward common prefixes and suffixes, like *un-*, *-ed* and *-ly*).\n",
    "\n",
    "Your job is to develop a *ngram* language model over characters, train this over the training words (being careful to handle the start of each word properly.) You should use linear interpolation to smooth between the higher order and lower order models, and you will have to decide how to weight each component. \n",
    "\n",
    "Your guessing AI algorithm should apply your language model to each blank position in the secret word by using as much of the left context as is known. E.g., in `_ e c _ e _ _` we know the full left context for the first blank, we have a context of two characters for the second blank, one character for the second last blank, and nothing for the last one. If we were using a *n=3* order model, we would be able to apply it to the first and second blanks, but would only be able to use the bigram or unigram distributions for the subsequent blanks. As with the unigram model, you should sum over the probability distributions for each blank, then select the highest probability character.\n",
    "\n",
    "Implement the ngram method for *n=3..5* and evaluate the performance on the test set. Do you see any improvement over the unigram methods above?\n",
    "\n",
    "(2 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unigram = defaultdict(float)\n",
    "count = 0\n",
    "for w in trainging_set:\n",
    "    new_w = w\n",
    "    for i in range(0, len(new_w)):\n",
    "        words = new_w[i]\n",
    "        unigram[words] = unigram.get(words,0) + 1\n",
    "        count = count + 1\n",
    "unigram_prob = defaultdict(float)\n",
    "for key in unigram:\n",
    "    unigram_prob[key] = unigram[key]/float(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the grams function below only compute the probablity of words \n",
    "bigram = defaultdict(float)\n",
    "count = 0\n",
    "for w in trainging_set:\n",
    "    new_w = \"#\"+w+\"&\"\n",
    "    for i in range(0, len(new_w)-1):\n",
    "        words = new_w[i:i+2]\n",
    "        bigram[words] = bigram.get(words,0) + 1\n",
    "        count = count + 1\n",
    "bigram_prob = defaultdict(float)\n",
    "for key in bigram:\n",
    "    bigram_prob[key] = bigram[key]/float(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trigram = defaultdict(float)\n",
    "count = 0\n",
    "\n",
    "for w in trainging_set:\n",
    "    new_w = \"@#\"+w+\"&*\"\n",
    "    for i in range(0, len(new_w)-2):\n",
    "        words = new_w[i:i+3]\n",
    "        trigram[words] = trigram.get(words,0) + 1\n",
    "        count = count + 1\n",
    "trigram_prob = defaultdict(float)\n",
    "for key in trigram:\n",
    "    trigram_prob[key] = trigram[key]/float(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fourgram = defaultdict(float)\n",
    "count = 0\n",
    "for w in trainging_set:\n",
    "    new_w = \"$@#\"+w+\"&*%\"\n",
    "    for i in range(0, len(new_w)-3):\n",
    "        words = new_w[i:i+4]\n",
    "        fourgram[words] = fourgram.get(words,0) + 1\n",
    "        count = count + 1\n",
    "fourgram_prob = defaultdict(float)\n",
    "for key in fourgram:\n",
    "    fourgram_prob[key] = fourgram[key]/float(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fivegram = defaultdict(float)\n",
    "count = 0\n",
    "for w in trainging_set:\n",
    "    new_w = \"!$@#\"+w+\"&*%^\"\n",
    "    for i in range(0, len(new_w)-4):\n",
    "        words = new_w[i:i+5]\n",
    "        fivegram[words] = fivegram.get(words,0) + 1\n",
    "        count = count + 1\n",
    "fivegram_prob = defaultdict(float)\n",
    "for key in fivegram:\n",
    "    fivegram_prob[key] = fivegram[key]/float(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a list to store the gram value \n",
    "def gram_dict(dict1,dict2,new_dict):\n",
    "    for key in dict2.keys():\n",
    "        pre_gram = key[:-1]\n",
    "        if dict1.get(pre_gram,0) == 0:\n",
    "            new_dict[key] = 0\n",
    "        else:\n",
    "            new_dict[key] = dict2[key]/dict1[pre_gram]\n",
    "\n",
    "con_bigram = defaultdict(float)\n",
    "con_trigram = defaultdict(float)\n",
    "con_fourgram = defaultdict(float)\n",
    "con_fifgram = defaultdict(float)\n",
    "\n",
    "gram_dict(unigram_prob,bigram_prob,con_bigram)\n",
    "gram_dict(bigram_prob,trigram_prob,con_trigram)\n",
    "gram_dict(trigram_prob,fourgram_prob,con_fourgram)\n",
    "gram_dict(fourgram_prob,fivegram_prob,con_fifgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute the '_'s and the two chars before '_'\n",
    "def trigram_model(mask, guessed):\n",
    "    para3 = 0.5\n",
    "    para2 = 0.3\n",
    "    para1= 0.2\n",
    "    guessed_list = list(guessed)\n",
    "    \n",
    "    new_mask = \"@#\"\n",
    "    for item in mask:\n",
    "        new_mask = new_mask + item\n",
    "    new_mask = new_mask+\"&*\"\n",
    "    \n",
    "    best_prob = 0\n",
    "    best_chr = ''\n",
    "    for j in range(0, 26):\n",
    "        this_chr = chr(j+97)\n",
    "        this_prob = 0\n",
    "        if this_chr in guessed_list: \n",
    "            continue\n",
    "        else:\n",
    "            for i in range(2, len(new_mask)):\n",
    "                if new_mask[i] == '_':\n",
    "                    tri_w = new_mask[i-2:i]+this_chr\n",
    "                    bi_w = new_mask[i-1:i]+this_chr\n",
    "                    uni_w = this_chr\n",
    "                    tri_prob = con_trigram.get(tri_w,0)\n",
    "                    bi_prob = con_bigram.get(bi_w,0)\n",
    "                    ui_prob = unigram_prob.get(uni_w,0)\n",
    "                    this_prob = para1*ui_prob + bi_prob*para2 + tri_prob*para3 + this_prob\n",
    "            if this_prob > best_prob:\n",
    "                best_prob = this_prob\n",
    "                best_chr = this_chr\n",
    "    return best_chr           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of mistakes of trigram is\n",
      "8.708\n"
     ]
    }
   ],
   "source": [
    "all_mistakes = 0\n",
    "for w in test_set:\n",
    "    mistakes = hangman(w, trigram_model, max_mistakes=26, verbose=False)\n",
    "    all_mistakes = all_mistakes + mistakes\n",
    "print (\"The average number of mistakes of trigram is\")\n",
    "print (float(all_mistakes)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# consider the '_'s and chars before '_'\n",
    "def fourgram_model(mask, guessed):\n",
    "    para4= 0.4\n",
    "    para3 = 0.3\n",
    "    para2 = 0.2\n",
    "    para1= 0.1\n",
    "    guessed_list = list(guessed)\n",
    "    new_mask = \"$@#\"\n",
    "    for item in mask:\n",
    "        new_mask = new_mask + item\n",
    "    new_mask = new_mask+\"&*%\"\n",
    "    \n",
    "    best_prob = 0\n",
    "    best_chr = ''\n",
    "    for j in range(0, 26):\n",
    "        this_chr = chr(j+97)\n",
    "        this_prob = 0\n",
    "        if this_chr in guessed_list: \n",
    "            continue\n",
    "        else:\n",
    "            for i in range(2, len(new_mask)):\n",
    "                if new_mask[i] == '_':\n",
    "                    four_w = new_mask[i-3:i]+this_chr\n",
    "                    tri_w = new_mask[i-2:i]+this_chr\n",
    "                    bi_w = new_mask[i-1:i]+this_chr\n",
    "                    uni_w = this_chr\n",
    "                    four_prob = con_trigram.get(four_w,0)\n",
    "                    tri_prob = con_trigram.get(tri_w,0)\n",
    "                    bi_prob = con_bigram.get(bi_w,0)\n",
    "                    ui_prob = unigram_prob.get(uni_w,0)    \n",
    "                    this_prob = para1*ui_prob + bi_prob*para2 + tri_prob*para3 + four_prob*para4 +this_prob\n",
    "            if this_prob > best_prob:\n",
    "                best_prob = this_prob\n",
    "                best_chr = this_chr\n",
    "    return best_chr       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of mistakes of fourgram is:\n",
      "8.716\n"
     ]
    }
   ],
   "source": [
    "all_mistakes = 0\n",
    "for w in test_set:\n",
    "    mistakes = hangman(w, fourgram_model, max_mistakes=26, verbose=False)\n",
    "    all_mistakes = all_mistakes + mistakes\n",
    "print (\"The average number of mistakes of fourgram is:\")\n",
    "print (float(all_mistakes)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# condider the '_'s and the chars before '_'\n",
    "def fivegram_model(mask, guessed):\n",
    "    para5 = 0.5\n",
    "    para4 = 0.4\n",
    "    para3 = 0.2\n",
    "    para2 = 0.08\n",
    "    para1= 0.02\n",
    "    guessed_list = list(guessed)\n",
    "    new_mask = \"!$@#\"\n",
    "    for item in mask:\n",
    "        new_mask = new_mask + item\n",
    "    new_mask = new_mask+\"&*%^\"\n",
    "    \n",
    "    best_prob = 0\n",
    "    best_chr = ''\n",
    "    for j in range(0, 26):\n",
    "        this_chr = chr(j+97)\n",
    "        this_prob = 0\n",
    "        if this_chr in guessed_list: \n",
    "            continue\n",
    "        else:\n",
    "            for i in range(2, len(new_mask)):\n",
    "                if new_mask[i] == '_':\n",
    "                    five_w = new_mask[i-4:i]+this_chr\n",
    "                    four_w = new_mask[i-3:i]+this_chr\n",
    "                    tri_w = new_mask[i-2:i]+this_chr\n",
    "                    bi_w = new_mask[i-1:i]+this_chr\n",
    "                    uni_w = this_chr\n",
    "                    five_prob = con_trigram.get(five_w,0)\n",
    "                    four_prob = con_trigram.get(four_w,0)\n",
    "                    tri_prob = con_trigram.get(tri_w,0)\n",
    "                    bi_prob = con_bigram.get(bi_w,0)\n",
    "                    ui_prob = unigram_prob.get(uni_w,0)    \n",
    "                    this_prob = para1*ui_prob + bi_prob*para2 + tri_prob*para3 + four_prob*para4 + five_prob*para5 + this_prob\n",
    "            if this_prob > best_prob:\n",
    "                best_prob = this_prob\n",
    "                best_chr = this_chr\n",
    "    return best_chr   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of mistakes of fivegram is:\n",
      "8.677\n"
     ]
    }
   ],
   "source": [
    "all_mistakes = 0\n",
    "for w in test_set:\n",
    "    mistakes = hangman(w, fivegram_model, max_mistakes=26, verbose=False)\n",
    "    all_mistakes = all_mistakes + mistakes\n",
    "print (\"The average number of mistakes of fivegram is:\")\n",
    "print (float(all_mistakes)/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Improving the AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** To get the extra credit, you should try to develop a more effective AI for hangman. Feel free to engage your creativity here! Possibilities include better conditioning on the length of the word and the parts that are known, fancier smoothing methods, using backwards ngram models, or a fancier inference algorithm. Ensure you report the test performance of your method.\n",
    "\n",
    "You will be marked based on the ambition of your approach and on its accuracy. If you have tried some truly spectacular method but it didn't really work, then please include your implementation and an explanation, which will still attract marks for ambition.\n",
    "\n",
    "(1 bonus mark) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# in this model, consider both the chars before '_' and chars after '_'. That means compute the grams forward and \n",
    "# backward. The result shows that it is effective.\n",
    "def rev_gram_dict(dict1,dict2,new_dict):\n",
    "    for key in dict2.keys():\n",
    "        pos_gram = key[1:]\n",
    "        if dict1.get(pos_gram,0) == 0:\n",
    "            new_dict[key] = 0\n",
    "        else:\n",
    "            new_dict[key] = dict2[key]/dict1[pos_gram]\n",
    "\n",
    "rev_con_bigram = defaultdict(float)\n",
    "rev_con_trigram = defaultdict(float)\n",
    "rev_con_fourgram = defaultdict(float)\n",
    "rev_con_fifgram = defaultdict(float)\n",
    "\n",
    "rev_gram_dict(unigram_prob,bigram_prob,rev_con_bigram)\n",
    "rev_gram_dict(bigram_prob,trigram_prob,rev_con_trigram)\n",
    "rev_gram_dict(trigram_prob,fourgram_prob,rev_con_fourgram)\n",
    "rev_gram_dict(fourgram_prob,fivegram_prob,rev_con_fifgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rev_tri_model(mask, guessed):\n",
    "    para3 = 0.5\n",
    "    para2 = 0.3\n",
    "    para1= 0.2\n",
    "    guessed_list = list(guessed)\n",
    "    new_mask = \"@#\"\n",
    "    for item in mask:\n",
    "        new_mask = new_mask + item\n",
    "    new_mask = new_mask+\"&*\"\n",
    "    \n",
    "    best_prob = 0\n",
    "    best_chr = ''\n",
    "    for j in range(0, 26):\n",
    "        this_chr = chr(j+97)\n",
    "        this_prob = 0\n",
    "        if this_chr in guessed_list: \n",
    "            continue\n",
    "        else:\n",
    "            for i in range(2, len(new_mask)):\n",
    "                if new_mask[i] == '_':\n",
    "                    tri_w = new_mask[i-2:i]+this_chr\n",
    "                    bi_w = new_mask[i-1:i]+this_chr\n",
    "                    uni_w = this_chr\n",
    "\n",
    "                    tri_prob = con_trigram.get(tri_w,0)\n",
    "                    bi_prob = con_bigram.get(bi_w,0)\n",
    "                    ui_prob = unigram_prob.get(uni_w,0)\n",
    "                    \n",
    "                    rev_tri_w = this_chr + new_mask[i+1:i+3]\n",
    "                    rev_bi_w = this_chr + new_mask[i+1:i+2]\n",
    "                    \n",
    "                    rev_tri_prob = con_trigram.get(rev_tri_w,0)\n",
    "                    re_bi_prob = con_bigram.get(rev_bi_w,0)\n",
    "                    ui_prob = unigram_prob.get(uni_w,0)\n",
    "                    \n",
    "                    p1 = para1*ui_prob+para2*bi_prob+para3*tri_prob\n",
    "                    p2 = para1*ui_prob+para2*re_bi_prob+para3*rev_tri_prob\n",
    "                    this_prob = p1 * p2\n",
    "            if this_prob > best_prob:\n",
    "                best_prob = this_prob\n",
    "                best_chr = this_chr\n",
    "    return best_chr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of mistakes of improved trigram is:\n",
      "8.012\n"
     ]
    }
   ],
   "source": [
    "all_mistakes = 0\n",
    "for w in test_set:\n",
    "    mistakes = hangman(w, rev_tri_model, max_mistakes=26, verbose=False)\n",
    "    all_mistakes = all_mistakes + mistakes\n",
    "print (\"The average number of mistakes of improved trigram is:\")\n",
    "print (float(all_mistakes)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rev_four_model(mask, guessed):\n",
    "    para4 = 0.5\n",
    "    para3 = 0.4\n",
    "    para2 = 0.08\n",
    "    para1= 0.02\n",
    "    guessed_list = list(guessed)\n",
    "    new_mask = \"$@#\"\n",
    "    for item in mask:\n",
    "        new_mask = new_mask + item\n",
    "    new_mask = new_mask+\"&*%\"\n",
    "    \n",
    "    best_prob = 0\n",
    "    best_chr = ''\n",
    "    for j in range(0, 26):\n",
    "        this_chr = chr(j+97)\n",
    "        this_prob = 0\n",
    "        if this_chr in guessed_list: \n",
    "            continue\n",
    "        else:\n",
    "            for i in range(2, len(new_mask)):\n",
    "                if new_mask[i] == '_':\n",
    "                    four_w = new_mask[i-3:i]+this_chr\n",
    "                    tri_w = new_mask[i-2:i]+this_chr\n",
    "                    bi_w = new_mask[i-1:i]+this_chr\n",
    "                    uni_w = this_chr\n",
    "                    \n",
    "                    four_prob = con_trigram.get(four_w,0)\n",
    "                    tri_prob = con_trigram.get(tri_w,0)\n",
    "                    bi_prob = con_bigram.get(bi_w,0)\n",
    "                    ui_prob = unigram_prob.get(uni_w,0)\n",
    "                    \n",
    "                    rev_four_w = this_chr + new_mask[i+1:i+4]\n",
    "                    rev_tri_w = this_chr + new_mask[i+1:i+3]\n",
    "                    rev_bi_w = this_chr + new_mask[i+1:i+2]\n",
    "                    \n",
    "                    rev_four_prob = con_trigram.get(rev_four_w,0)\n",
    "                    rev_tri_prob = con_trigram.get(rev_tri_w,0)\n",
    "                    re_bi_prob = con_bigram.get(rev_bi_w,0)\n",
    "                    ui_prob = unigram_prob.get(uni_w,0)\n",
    "                    \n",
    "                    p1 = para1*ui_prob+para2*bi_prob+para3*tri_prob+para4*four_prob\n",
    "                    p2 = para1*ui_prob+para2*re_bi_prob+para3*rev_tri_prob+para4*rev_four_prob\n",
    "                    this_prob = p1 * p2\n",
    "            if this_prob > best_prob:\n",
    "                best_prob = this_prob\n",
    "                best_chr = this_chr\n",
    "    return best_chr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of mistakes of improved fourigram is:\n",
      "8.001\n"
     ]
    }
   ],
   "source": [
    "all_mistakes = 0\n",
    "for w in test_set:\n",
    "    mistakes = hangman(w, rev_four_model, max_mistakes=26, verbose=False)\n",
    "    all_mistakes = all_mistakes + mistakes\n",
    "print (\"The average number of mistakes of improved fourigram is:\")\n",
    "print (float(all_mistakes)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rev_five_model(mask, guessed):\n",
    "    para5 = 0.5\n",
    "    para4 = 0.4\n",
    "    para3 = 0.2\n",
    "    para2 = 0.08\n",
    "    para1= 0.02\n",
    "    guessed_list = list(guessed)\n",
    "    new_mask = \"!$@#\"\n",
    "    for item in mask:\n",
    "        new_mask = new_mask + item\n",
    "    new_mask = new_mask+\"&*%^\"\n",
    "    \n",
    "    best_prob = 0\n",
    "    best_chr = ''\n",
    "    for j in range(0, 26):\n",
    "        this_chr = chr(j+97)\n",
    "        this_prob = 0\n",
    "        if this_chr in guessed_list: \n",
    "            continue\n",
    "        else:\n",
    "            for i in range(2, len(new_mask)):\n",
    "                if new_mask[i] == '_':\n",
    "                    five_w = new_mask[i-4:i]+this_chr\n",
    "                    four_w = new_mask[i-3:i]+this_chr\n",
    "                    tri_w = new_mask[i-2:i]+this_chr\n",
    "                    bi_w = new_mask[i-1:i]+this_chr\n",
    "                    uni_w = this_chr\n",
    "                    \n",
    "                    five_prob = con_trigram.get(five_w,0)\n",
    "                    four_prob = con_trigram.get(four_w,0)\n",
    "                    tri_prob = con_trigram.get(tri_w,0)\n",
    "                    bi_prob = con_bigram.get(bi_w,0)\n",
    "                    ui_prob = unigram_prob.get(uni_w,0)\n",
    "                    \n",
    "                    rev_five_w = this_chr + new_mask[i+1:i+5]\n",
    "                    rev_four_w = this_chr + new_mask[i+1:i+4]\n",
    "                    rev_tri_w = this_chr + new_mask[i+1:i+3]\n",
    "                    rev_bi_w = this_chr + new_mask[i+1:i+2]\n",
    "                    \n",
    "                    rev_five_prob = con_trigram.get(rev_five_w,0)\n",
    "                    rev_four_prob = con_trigram.get(rev_four_w,0)\n",
    "                    rev_tri_prob = con_trigram.get(rev_tri_w,0)\n",
    "                    re_bi_prob = con_bigram.get(rev_bi_w,0)\n",
    "                    ui_prob = unigram_prob.get(uni_w,0)\n",
    "                    \n",
    "                    p1 = para1*ui_prob+para2*bi_prob+para3*tri_prob+para4*four_prob+para5*five_prob\n",
    "                    p2 = para1*ui_prob+para2*re_bi_prob+para3*rev_tri_prob+para4*rev_four_prob+para5*rev_five_prob\n",
    "                    this_prob = p1 * p2\n",
    "            if this_prob > best_prob:\n",
    "                best_prob = this_prob\n",
    "                best_chr = this_chr\n",
    "    return best_chr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of mistakes of improved fivegram\n",
      "7.97\n"
     ]
    }
   ],
   "source": [
    "all_mistakes = 0\n",
    "for w in test_set:\n",
    "    mistakes = hangman(w, rev_five_model, max_mistakes=26, verbose=False)\n",
    "    all_mistakes = all_mistakes + mistakes\n",
    "print (\"The average number of mistakes of improved fivegram\")\n",
    "print (float(all_mistakes)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
