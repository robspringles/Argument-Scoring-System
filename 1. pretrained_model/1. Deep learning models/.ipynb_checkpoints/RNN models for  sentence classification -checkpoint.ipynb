{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN models for sentence classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Simple LSTM for Sequence Classification\n",
    "* LSTM For Sequence Classification With Dropout\n",
    "* LSTM and Convolutional Neural Network For Sequence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('/Users/apple/Documents/GitHub/Argument-Scoring-System/1. pretrained_model/labeled_essay_dics.pickle', 'rb') as handle:\n",
    "    label_sents = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute and store and the {sents: label} into one dic\n",
    "all_label_sents = {}\n",
    "for key in label_sents.keys():\n",
    "    this_essay = label_sents[key]\n",
    "    for s in this_essay.keys():\n",
    "        all_label_sents[s] = this_essay[s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Original taken from https://github.com/dennybritz/cnn-text-classification-tf\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clear_split_str(s):\n",
    "    strip_s = s.strip()\n",
    "    clear_s = clean_str(strip_s)\n",
    "    s_text = clear_s.split(\" \")\n",
    "    return s_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "for key in all_label_sents.keys():\n",
    "    this_value = all_label_sents[key]\n",
    "    clear_key = clear_split_str(key)\n",
    "    texts = texts + clear_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7701\n"
     ]
    }
   ],
   "source": [
    "print (len(set(texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## >> ? need to choose the top 5000 words here \n",
    "# from nltk import FreqDist\n",
    "# fdist1 = FreqDist(texts)\n",
    "# texts = [w for w in fdist1 if fdist1[w] > 10]\n",
    "# # print (texts)\n",
    "# print (len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = sorted(set(texts))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoded_txt(text):\n",
    "    return np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## >> remove low frequency words \n",
    "# def encoded_txt(text):\n",
    "#     test = []\n",
    "#     for item in text:\n",
    "#         if item in vocab_to_int:\n",
    "#             test.append(vocab_to_int[item])\n",
    "#     return np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear data and store in one dictionary\n",
    "X = []\n",
    "encode_X = []\n",
    "y = []\n",
    "for key in all_label_sents.keys():\n",
    "    this_value = all_label_sents[key]\n",
    "    clear_key = clear_split_str(key)\n",
    "    X.append(clear_key)\n",
    "    encode_X.append(encoded_txt(clear_key))\n",
    "    y.append(this_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Empty', 'Premise', 'Claim', 'MajorClaim', 'Empty', 'Empty', 'Premise', 'Empty', 'Premise', 'Claim', 'Claim', 'Empty', 'Claim', 'Premise', 'Premise', 'Premise', 'Empty', 'Premise', 'MajorClaim', 'Claim', 'Premise', 'Claim', 'Claim', 'Claim', 'Empty', 'Premise', 'Premise', 'Empty', 'Premise', 'Claim', 'Empty', 'Premise', 'Premise', 'Premise', 'Empty', 'Claim', 'Empty', 'Empty', 'Empty', 'Empty', 'Claim', 'Premise', 'Empty', 'Premise', 'Claim', 'Empty', 'Premise', 'MajorClaim', 'Premise', 'MajorClaim', 'Empty', 'Empty', 'MajorClaim', 'MajorClaim', 'Premise', 'Premise', 'Empty', 'Empty', 'Premise', 'Empty', 'Premise', 'Premise', 'Premise', 'Empty', 'MajorClaim', 'Premise', 'Premise', 'Premise', 'Premise', 'Premise', 'MajorClaim', 'Empty', 'Premise', 'Premise', 'Empty', 'Premise', 'Premise', 'Claim', 'Premise', 'Empty', 'Premise', 'Empty', 'MajorClaim', 'Premise', 'Claim', 'Premise', 'Claim', 'Premise', 'Premise', 'Empty', 'Premise', 'Premise', 'Claim', 'Premise', 'Premise', 'MajorClaim', 'Claim', 'Empty', 'Empty']\n"
     ]
    }
   ],
   "source": [
    "print (y[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encode_y = []\n",
    "for s in y:\n",
    "    if s == 'Claim':\n",
    "        encode_y.append([1,0,0,0])\n",
    "    elif s == 'Premise':\n",
    "        encode_y.append([0,1,0,0])\n",
    "    elif s == 'Empty':\n",
    "        encode_y.append([0,0,1,0])\n",
    "    else: \n",
    "        encode_y.append([0,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "length = int(len(X) * 0.9)\n",
    "X_train = encode_X[0:length]\n",
    "y_train = encode_y[0:length]\n",
    "X_test = encode_X[length:]\n",
    "y_test = encode_y[length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6369 6369 708 708\n"
     ]
    }
   ],
   "source": [
    "print (len(X_train), len(y_train), len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3530   74 4727   19  363 2786  880 4051 1809 4519 3634 4052   19 6364  485\n",
      "  830 6894 6593 3530 6894 1177  400 7493 6894 6594 6976] [0, 1, 0, 0]\n",
      "[3530   74 4727   19  363 2786  880 4051 1809 4519 3634 4052   19 6364  485\n",
      "  830 6894 6593 3530 6894 1177  400 7493 6894 6594 6976] [0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print (encode_X[1000], encode_y[1000])\n",
    "print (X_train[1000], y_train[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length is: 72\n"
     ]
    }
   ],
   "source": [
    "max_lenth = 0\n",
    "for item in X:\n",
    "    if len(item) > max_lenth:\n",
    "        max_lenth = len(item)\n",
    "\n",
    "print (\"max sentence length is: \" + str(max_lenth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple LSTM for Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "# from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "max_sent_length = 72\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_sent_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_sent_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "# top_words = 7701\n",
    "top_words = len(vocab)\n",
    "# (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 72, 32)            246432    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 300,036\n",
      "Trainable params: 300,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 6369 samples, validate on 708 samples\n",
      "Epoch 1/3\n",
      "6369/6369 [==============================] - 31s - loss: 0.5284 - acc: 0.7459 - val_loss: 0.5028 - val_acc: 0.7500\n",
      "Epoch 2/3\n",
      "6369/6369 [==============================] - 30s - loss: 0.4797 - acc: 0.7870 - val_loss: 0.4516 - val_acc: 0.8016\n",
      "Epoch 3/3\n",
      "3136/6369 [=============>................] - ETA: 14s - loss: 0.4056 - acc: 0.8265"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(top_words, embedding_vecor_length, input_length=max_sent_length))\n",
    "model1.add(LSTM(100))\n",
    "model1.add(Dense(4, activation='sigmoid'))\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model1.summary())\n",
    "model1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model1.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LSTM For Sequence Classification With Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(top_words, embedding_vecor_length, input_length=max_sent_length))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(LSTM(100))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(4, activation='sigmoid'))\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model2.summary())\n",
    "model2.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores2 = model2.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores2[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM and Convolutional Neural Network For Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(top_words, embedding_vecor_length, input_length=max_sent_length))\n",
    "model3.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model3.add(MaxPooling1D(pool_size=2))\n",
    "model3.add(LSTM(100))\n",
    "model3.add(Dense(4, activation='sigmoid'))\n",
    "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model3.summary())\n",
    "model3.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores3 = model3.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores3[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "embedding_dim = 300\n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Prepossessing parameters\n",
    "sequence_length = 72\n",
    "max_words = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "input_shape = (sequence_length,)\n",
    "model_input = Input(shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = Embedding(top_words, embedding_vecor_length, input_length=max_sent_length)(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = Dropout(dropout_prob[0])(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolutional block\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Convolution1D(filters=num_filters,\n",
    "                         kernel_size=sz,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1)(z)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "\n",
    "z = Dropout(dropout_prob[1])(z)\n",
    "z = Dense(hidden_dims, activation=\"relu\")(z)\n",
    "model_output = Dense(4, activation=\"sigmoid\")(z)\n",
    "\n",
    "model4 = Model(model_input, model_output)\n",
    "model4.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model4.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_data=(X_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores4 = model4.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores4[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comment data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* clear labeled data into sentence vectors \n",
    "* clear comment data into sentence vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a preprocessing function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent2vec(sent):\n",
    "    text = clear_split_str(sent)\n",
    "    return np.array([vocab_to_int[c] for c in text if c in vocab_to_int], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_labels(s):\n",
    "    this_label = []\n",
    "    if s == 'Claim':\n",
    "        this_label = [1,0,0,0]\n",
    "    elif s == 'Premise':\n",
    "        this_label = [0,1,0,0]\n",
    "    elif s == 'Empty':\n",
    "        this_label = [0,0,1,0]\n",
    "    else: \n",
    "        this_label = [0,0,0,1]\n",
    "    return this_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load comment data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load labeled comment data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comment_path = 'comment_sent.csv'\n",
    "label_comments_data = pd.read_csv(comment_path,encoding = \"ISO-8859-1\")\n",
    "label_sents = label_comments_data['sentence']\n",
    "label_components = label_comments_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '/Users/apple/Documents/GitHub/Argument-Scoring-System/comment_data/comments.csv'\n",
    "data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = data['comment_text']\n",
    "comment_scores = data['mean_evaluation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (len(comment_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit the model on the labeled data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generating vectors \n",
    "new_X = [sent2vec(item) for item in label_sents]\n",
    "new_X = sequence.pad_sequences(new_X, maxlen=max_sent_length)\n",
    "new_y = [sent_labels(item) for item in label_components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_X_train = new_X[0:50]\n",
    "new_y_train = new_y[0:50]\n",
    "new_X_test = new_X[50:]\n",
    "new_y_test = new_y[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict the component of sentences in comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict with the original model\n",
    "# calculate the accuracy\n",
    "# Final evaluation of the model\n",
    "scores = model1.evaluate(new_X_test, new_y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predit with the fitted model \n",
    "# calculate the accuracy \n",
    "model1.fit(new_X_train, new_y_train, validation_data=(new_X_test, new_y_test), epochs=3, batch_size=64)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model1.evaluate(new_X_test, new_y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate a vector of components for each comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "def get_max_index(vector):\n",
    "    max_value = max(vector)\n",
    "    for i in range(4):\n",
    "        if max_value == vector[i]:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write a function to generate the vector for each components of comments \n",
    "from nltk.tokenize import sent_tokenize\n",
    "def components_text(text):\n",
    "    sents = sent_tokenize(text)\n",
    "    sents_vector = [sent2vec(s) for s in sents]\n",
    "    sents_vector = sequence.pad_sequences(sents_vector, maxlen=max_sent_length)\n",
    "#     change the deep learning model here\n",
    "    components = model4.predict(sents_vector)\n",
    "    return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return the argument components of a comment \n",
    "def get_components(text):\n",
    "    vectors = components_text(text)\n",
    "    components = [get_max_index(v) for v in vectors]\n",
    "    return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return the number of words for every sentence \n",
    "def sent_words(text):\n",
    "    sents = sent_tokenize(text)\n",
    "    word_count = []\n",
    "    for s in sents:\n",
    "        word_count.append(len(s))\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 22\n",
    "# print (texts[i])\n",
    "# print (comment_scores[i])\n",
    "# print (components_text(texts[i]))\n",
    "print (get_components(texts[i]))\n",
    "print (sent_words(texts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# natio of claim and other components\n",
    "def component_num(text):\n",
    "    components = get_components(text)\n",
    "    Claim_num = 0\n",
    "    Premise_num = 0\n",
    "    Empty_num = 0\n",
    "    major_claim = 0 \n",
    "    \n",
    "    for item in components:\n",
    "        if item == 0:\n",
    "            Claim_num += 1\n",
    "        elif item == 1:\n",
    "            Premise_num += 1\n",
    "        elif item == 2:\n",
    "            Empty_num +=1 \n",
    "        else:\n",
    "            major_claim += 1\n",
    "            \n",
    "    return [Claim_num, Premise_num, Empty_num, major_claim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (component_num(texts[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** natio for components ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return the ratio for every element\n",
    "def component_ratio(text):\n",
    "    components = component_num(text)\n",
    "    com_sum = sum(components)\n",
    "    \n",
    "    natio_list = []\n",
    "    for item in components:\n",
    "        temp = float(item)/com_sum\n",
    "        natio_list.append(round(temp,2))\n",
    "    return natio_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (component_ratio(texts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# words count of these components \n",
    "def component_word_count(text):\n",
    "    # the component list and corresponding sentence word count \n",
    "    comp_list = get_components(text)\n",
    "    words_num_list = sent_words(text)\n",
    "    \n",
    "    Claim_num = 0\n",
    "    Premise_num = 0\n",
    "    Empty_num = 0\n",
    "    major_claim = 0 \n",
    "    \n",
    "    for i in range(len(comp_list)):\n",
    "        temp = comp_list[i]\n",
    "        if temp == 0:\n",
    "            Claim_num += words_num_list[i]\n",
    "        elif temp == 1:\n",
    "            Premise_num += words_num_list[i]\n",
    "        elif temp == 2:\n",
    "            Empty_num += words_num_list[i]\n",
    "        else:\n",
    "            major_claim += words_num_list[i]\n",
    "\n",
    "    return [Claim_num, Premise_num, Empty_num, major_claim]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (component_word_count(texts[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ***natio of words number for every component***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word natio for every component\n",
    "def component_word_natio(text):\n",
    "    components_words_list = component_word_count(text)\n",
    "    word_sum = sum(components_words_list)\n",
    "    \n",
    "    natio_list = []\n",
    "    for item in components_words_list:\n",
    "        temp = float(item)/word_sum\n",
    "        natio_list.append(round(temp,2))\n",
    "    return natio_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (component_word_natio(texts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# texts = data['comment_text']\n",
    "# comment_scores = data['mean_evaluation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i in range(100):\n",
    "# #     print (component_num(texts[i])\n",
    "#     print (texts[i])\n",
    "#     print (component_num(texts[i]))\n",
    "#     print (component_ratio(texts[i]))\n",
    "#     print (component_word_count(texts[i]))\n",
    "#     print (component_word_natio(texts[i]))\n",
    "#     print (comment_scores[i])\n",
    "#     print ('-----')\n",
    "#     print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store the result into a pickle file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize the number of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "component_nums = []\n",
    "for i in range(len(texts)):\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ***the number of component*** VS ***score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the ratio of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
