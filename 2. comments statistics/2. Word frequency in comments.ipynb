{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word frequency analyze "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words like \"dogs, gay and rubbish\" is more frequent in low score comments. Although some comments are long and well strucured, but due to the insulting words, the score becomes low. So this code is analyzing the word frequency to see if the word frequency is different in different comments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/Users/apple/Documents/GitHub/Argument-Scoring-System/comment_data/comments.csv'\n",
    "data = pd.read_csv(path)\n",
    "text = data['comment_text']\n",
    "score = data['mean_evaluation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print (text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Original taken from https://github.com/dennybritz/cnn-text-classification-tf\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clear_split_str(s):\n",
    "    strip_s = s.strip()\n",
    "    clear_s = clean_str(strip_s)\n",
    "    s_text = clear_s.split(\" \")\n",
    "    return s_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## clear text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenize words\n",
    "clean_text = [clear_split_str(sent) for sent in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'gearing', 'may', 'have', 'seemed', 'like', 'a', 'good', 'idea', 'when', 'it', 'was', 'first', 'introduced', ',', 'but', 'it', 'has', 'become', 'little', 'more', 'than', 'a', 'massive', 'rort', 'the', 'only', 'reason', 'we', 'still', 'have', 'it', 'after', 'its', 'problems', 'have', 'been', 'pointed', 'out', 'so', 'often', 'is', 'that', 'politicians', 'are', 'under', 'the', 'thumb', 'of', 'the', '20', 'who', 'do', 'most', 'of', 'the', 'negative', 'gearing', 'and', 'have', 'the', 'power', 'to', 'scream', 'the', 'loudest', 'come', 'election', 'time', 'it', 'has', 'no', 'place', 'in', 'a', 'just', 'and', 'efficient', 'economic', 'system']\n"
     ]
    }
   ],
   "source": [
    "print (clean_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## dive data into four parts\n",
    "* high score, low word count \n",
    "* low score, low word count\n",
    "* high score, high word count\n",
    "* high score, high word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# seperate based on word count and score\n",
    "# differences between different score\n",
    "score_dic  = defaultdict(list)\n",
    "for i in range(len(text)):\n",
    "    score_dic[score[i]] = score_dic.get(score[i], []) + clean_text[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set of words dictionary\n",
    "score_word_dic = {}\n",
    "for key in score_dic.keys():\n",
    "    score_word_dic[key] = set(score_dic[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, nan, nan, 2.3333333333333304, nan, nan, 3.6666666666666696, nan, nan, 4.5, nan, nan, nan, nan, nan, nan, 6.3333333333333304, nan, nan, nan, nan, nan, nan, nan, nan, nan, 5.6666666666666696, nan, 6.6666666666666696, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.3333333333333299, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 3.3333333333333304, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 4.3333333333333304, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 5.3333333333333304, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 4.6666666666666696, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 2.6666666666666696, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.6666666666666698, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n"
     ]
    }
   ],
   "source": [
    "print (score_word_dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# distinctive words for different score\n",
    "dist_score_word_dic = {}\n",
    "keys = {0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0}\n",
    "for key in keys:\n",
    "    key_temp = keys - {key}\n",
    "    value_temp = score_dic[key]\n",
    "    dist_score_word_dic[key] = value_temp\n",
    "    for v in key_temp:\n",
    "        dist_score_word_dic[key] = list(set(dist_score_word_dic[key]) - set(score_dic[v]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print (dist_score_word_dic[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0}\n"
     ]
    }
   ],
   "source": [
    "print (keys - {0.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There is no obvious features in words for different score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## most common words in different score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def freq_words(words, number):\n",
    "    '''\n",
    "    return the words which the frequency is higher than the given number\n",
    "    number: the lowest value of frequency\n",
    "    words: the cleared words list\n",
    "    '''\n",
    "    word_freq = FreqDist(words).most_common(1000)\n",
    "    words_list = [key for (key, value) in word_freq if value > number]\n",
    "    return words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fre_dict = {}\n",
    "for key in score_dic.keys():\n",
    "    fre_dict[key] = freq_words(score_dic[key], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys = {0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0}\n",
    "distinc_freq_dict = {}\n",
    "for key in keys:\n",
    "    key_temp = keys - {key}\n",
    "    value_temp = fre_dict[key]\n",
    "    distinc_freq_dict[key] = value_temp\n",
    "    for v in key_temp:\n",
    "        distinc_freq_dict[key] = list(set(distinc_freq_dict[key]) - set(fre_dict[v]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "['nukes']\n",
      "------\n",
      "1.0\n",
      "['abbot', 'father', 'voters', 'mothers', 'stupid', 'protected', 'says', 'stand', 'cant']\n",
      "------\n",
      "2.0\n",
      "['liberals', 'trusted', 'student', 'biological']\n",
      "------\n",
      "3.0\n",
      "['reporting', 'maternity', 'mixed', 'secure', 'line', 'tradition', 'mr', 'alternatives', 'speech', 'press', 'firstly', 'plans', 'days', 'gone', 'attention', 'leader', 'acceptance', 'solutions', 'federal', 'wear', 'campaign', 'learn', 'prime', 'minister', 'hours', 'hold', 'irrelevant', 'household', 'uses', 'mostly', 'earn', 'notion', 'divorce', 'throughout', 'performance', 'direction', 'defense', 'politician', 'tell', 'treated', '7', 'personally', 'main', 'yourself', 'highest']\n",
      "------\n",
      "4.0\n",
      "['success', 'distribution', 'implement', 'exam', 'substances', 'discriminate', 'dole', 'offend', 'period', 'depends', 'pass', 'camps', 'medium', 'usage', 'competitive', 'daily', 'deaths', 'told', 'goal', 'stay', 'teach', 'according', 'childcare', 'houston', 'university', 'belief', 'leaving', 'unproductive', 'payment', 'status', 'message', 'sydney', 'extra', '2012', 'requires', 'sake', 'passed', 'homes', 'break', 'socially', 'outside', 'parent', 'thousands', 'humans', 'wanting', 'island', 'eg', 'takes', 'adequate', 'heard', '200', 'maintain', 'smugglers', 'books', 'studies', 'queue', 'png', 'extreme', 'remove', 'unhcr', 'remember', 'study', 'enjoy', 'deliver', 'politically', 'stable', 'cup', '90', 'colour', 'perfect', 'greatest', 'failure', 'adult', 'acceptable', 'vast', 'justice']\n",
      "------\n",
      "5.0\n",
      "['requirements', 'plant', 'neither', 'unlawful', 'recognise', 'links', 'html', 'seeing', 'co', 'obligation', 'focus', 'emission', 'billions', 'stock', 'statement', 'construction', 'prepared', 'addicts', 'capital', 'trading', 'proven', 'managed', 'disagree', 'storage', 'failing', 'quickly', 'amounts', 'uk', 'news', 'manage', 'legally', 'wars', 'effort', 'engage', 'christian', 'determine', 'compete', 'size', 'essential', 'jo', 'friends', 'acknowledge', 'confession', 'outcome', 'practices', 'node', 'figures', 'brought', 'speed', 'regulation', 'crimes', 'invest', 'options', 'acting', 'moment', 'supposed', 'hardly', 'behind', 'acl', 'safer', 'capable', 'half', 'recognition', 'ban', 'european', 'correct', 'simple', 'deny', '18c', 'changing', 'city', 'expressed', 'cutting', 'decide', 'efforts', 'smaller', 'contract', 'purpose', 'naplan', '6', 'greens', 'additional', 'speeds', 'ship', 'ie', 'easier', 'area', 'sector', 'raised', 'visa', 'rising', 'debt', 'taxation', 'occur', 'cuts', 'air', 'worked', 'labour', '8', 'she', 'genuine', 'established', 'damaging', 'entirely', 'interested', 'equally', 'gives', 'cheap', 'lnp', 'produce', 'close', 'standard', 'copper', 'confessional', 'boom', 'incentives', 'elsewhere', 'century']\n",
      "------\n",
      "6.0\n",
      "['terminal', 'fight', 'reach', 'represent', 'america', 'story', 'protest', 'immigrants', 'false', 'violent', 'removing', 'participation', 'creating', 'incompetence', 'contribute', 'smoking', 'humane', 'eliminate', 'project', 'ng', 'dependent', 'primary', 'india', 'addictive', 'death', 'mortgage', 'farmers', 'white', 'ethnic', 'increasingly', 'rapidly', 'cases', 'separate', 'targets', 'left', 'providing', 'importantly', 'practice', 'democratic', 'places', 'persecution', 'exploitation', 'consumer', 'goods', 'decriminalisation', 'funded', 'reduced', 'principle', 'denying', 'product', 'decisions', 'abattoir', 'loss', 'gillard', 'parliament', 'readily', 'model', 'comments', 'indonesia', 'incompetent']\n",
      "------\n",
      "7.0\n",
      "['mental', 'concerns', 'humanitarian', 'compensation', 'sell', 'significantly', 'domestic', 'intended', 'encouraging', 'recomendations', 'cleaner', 'meet', 'existing', 'tons', 'possibly', 'wallace', 'generally', '50', 'regard', 'arrivals', 'decision', 'attempt', 'enforcement', 'abc', 'ultimately', 'substantial', 'obligations', 'maritime', 'experts', 'comment', 'harmful', 'criminalisation', 'base', 'activity', 'expected', 'payments', 'polygamy', 'doctor', 'relative', 'presented', 'third', 'planning', 'various', 'except', 'organisations', 'burden', 'design', 'producers', 'detailed', 'purchase', 'related', 'avoid', 'households', 'wonder', 'forced', 'committed', 'expert', 'corrupt', 'disease', 'essentially', 'regional', 'don', 'finally', 'scientists', 'affordable', 'availability', 'addressed', 'mdma', 'positive', 'ly', 'priority', 'journalism', 'mainly', 'led', 'user', 'repeal', 'analysis', 'consumers', 'jim', 'ought', 'b', 'largest', 'implemented', 'rent', 'creates', 'seeker', 'uranium', 'several', 'fleeing', 'unable', 'investments', 'factors', 'relatively', 'generate']\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# show the dinstinctive words in for different score\n",
    "for k in distinc_freq_dict:\n",
    "    print (k)\n",
    "    print (distinc_freq_dict[k])\n",
    "    print (\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# words for different score\n",
    "# for k in fre_dict:\n",
    "#     print (k)\n",
    "#     print (fre_dict[k])\n",
    "#     print (\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Findings\n",
    "* There is no findings for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
